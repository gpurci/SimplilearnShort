{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yyI1I49-A8-"
   },
   "source": [
    "**DESCRIPTION**\n",
    "\n",
    "**Problem Statement**\n",
    "\n",
    "Amazon is an online shopping website that now caters to millions of people everywhere. Over 34,000 consumer reviews for Amazon brand products like Kindle, Fire TV Stick and more are provided.\n",
    "\n",
    "The dataset has attributes like brand, categories, primary categories, reviews.title, reviews.text, and the sentiment. Sentiment is a categorical variable with three levels \"Positive\", \"Negative“, and \"Neutral\". For a given unseen data, the sentiment needs to be predicted.\n",
    "\n",
    "You are required to predict Sentiment or Satisfaction of a purchase based on multiple features and review text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "doa2gSCNhi-x",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "F8rtzB-Bhmyu"
   },
   "outputs": [],
   "source": [
    "#Import the necessary library\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "# import required libraries\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FmT4XKVYhK8l"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-11 22:05:47.050470: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-11 22:05:47.057768: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754939147.066072   74899 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754939147.068492   74899 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754939147.074914   74899 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754939147.074923   74899 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754939147.074924   74899 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754939147.074924   74899 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-11 22:05:47.077490: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics, svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from scikeras.wrappers import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NFYvbs_sJKs_"
   },
   "outputs": [],
   "source": [
    "from scipy import stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xx4HeU3UDOw3",
    "outputId": "e685f926-4e91-4238-9f7b-dc65a461e25e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/gheorghe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/gheorghe/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/gheorghe/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/gheorghe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/gheorghe/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from string import punctuation\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MV6LZ0XsDO1-"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "q-tJ0fOZAsZX"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, SMOTEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qX8rPNst9fzN"
   },
   "outputs": [],
   "source": [
    "from livelossplot import PlotLossesKerasTF\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Input, Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnNz6asOgw6z"
   },
   "source": [
    "# Data aquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rvBJu0-pgx-Y"
   },
   "outputs": [],
   "source": [
    "\n",
    "pd_df_train_data = pd.read_csv(\"/content/train_data.csv\")\n",
    "pd_df_test_data = pd.read_csv(\"/content/test_data.csv\")\n",
    "pd_df_test_data_hidden = pd.read_csv(\"/content/test_data_hidden.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vndnjqPGlGVr",
    "outputId": "d4b15fbe-ac60-44c7-85c4-a5c63ac4df8f"
   },
   "outputs": [],
   "source": [
    "pd_df_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dYY-3vAulRj8",
    "outputId": "8f8bcf6c-9d0e-49a7-b552-f291d907c17d"
   },
   "outputs": [],
   "source": [
    "pd_df_test_data_hidden.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m15CpSEZmQbH",
    "outputId": "8edfee92-4ac1-4050-b7f8-118ae2d0e93e"
   },
   "outputs": [],
   "source": [
    "pd_df_train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5gjMyJfjNxp"
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RYTgfZfzjUKL"
   },
   "outputs": [],
   "source": [
    "def print_performance(labels, predictions):\n",
    "  print(\"Accuracy = {}\".format(accuracy_score(labels, predictions)))\n",
    "  print(\"Precision = {}\".format(precision_score(labels, predictions)))\n",
    "  print(\"Recall = {}\".format(recall_score(labels, predictions)))\n",
    "  (tp, fp), (fn, tn)  = confusion_matrix(labels, predictions)\n",
    "  print(\"Confusion matrix: tp {}, fp {}, fn {}, tn {}\".format(tp, fp, fn, tn))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SCUMfDTIIDY"
   },
   "outputs": [],
   "source": [
    "def pd_df_multi_class_confusion_matrix(pd_s_target, pd_s_predict):\n",
    "  class_sample_ = pd_s_target.unique()\n",
    "  cm = confusion_matrix(pd_s_target, pd_s_predict, labels=class_sample_)\n",
    "  multi_columns = zip(['Predicted label']*(len(class_sample_)), class_sample_)\n",
    "  multi_index = zip(['Actual label']*(len(class_sample_)), class_sample_)\n",
    "  multi_columns = pd.MultiIndex.from_tuples(list(multi_columns))\n",
    "  multi_index = pd.MultiIndex.from_tuples(list(multi_index))\n",
    "  return pd.DataFrame(cm, columns=multi_columns, index=multi_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0qKG9G-_732"
   },
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "def roc_curve_multiclass(pd_s_target, pd_s_predict):\n",
    "  fpr = dict()\n",
    "  tpr = dict()\n",
    "  roc_auc = dict()\n",
    "  class_samples_ = pd_s_target.unique()\n",
    "  # Binarize the output\n",
    "  np_target = label_binarize(pd_s_target, classes=class_samples_)\n",
    "  np_predict = label_binarize(pd_s_predict, classes=class_samples_)\n",
    "\n",
    "  for sample, unique in zip(class_samples_, range(len(class_samples_))):\n",
    "    fpr[sample], tpr[sample], _ = roc_curve(np_target[:, unique], np_predict[:, unique])\n",
    "    roc_auc[sample] = auc(fpr[sample], tpr[sample])\n",
    "  return fpr, tpr, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yqZysGpPABXO"
   },
   "outputs": [],
   "source": [
    "def plot_auc_roc_multiclass(fpr, tpr, roc_auc, class_samples_):\n",
    "  plt.figure()\n",
    "  lw = len(class_samples_)\n",
    "  for i in class_samples_:\n",
    "      plt.plot(\n",
    "          fpr[i],\n",
    "          tpr[i],\n",
    "          lw=lw,\n",
    "          label=\"ROC curve of class {0} (area = {1:0.2f})\".format(i, roc_auc[i]),\n",
    "      )\n",
    "\n",
    "  plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "  plt.xlim([0.0, 1.0])\n",
    "  plt.ylim([0.0, 1.05])\n",
    "  plt.xlabel(\"False Positive Rate\")\n",
    "  plt.ylabel(\"True Positive Rate\")\n",
    "  plt.title(\"Receiver operating characteristic example\")\n",
    "  plt.legend(loc=\"lower right\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VEDm9w0ZE4Ju"
   },
   "outputs": [],
   "source": [
    "class MyTextProcess():\n",
    "  def __init__(self):\n",
    "    re_exp_punctuation = '[{}]'.format('\\\\'.join([char_ for char_ in punctuation]))\n",
    "    self.reObjPunct = re.compile(re_exp_punctuation)\n",
    "    self.reObjWhiteSpace = re.compile(r'\\s{2, 10}')\n",
    "\n",
    "    self.wnl = WordNetLemmatizer()\n",
    "\n",
    "  def txt_vectorization(self, sequence):\n",
    "    sequence = sequence.lower()\n",
    "    sequence = self.reObjPunct.sub(' ', sequence)\n",
    "    sequence = self.reObjWhiteSpace.sub(' ', sequence)\n",
    "    wordslist = nltk.word_tokenize(sequence)\n",
    "    wordslist = [self.wnl.lemmatize(word) for word in wordslist if word not in stopwords.words('english')]\n",
    "    return wordslist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YRZmHyYBJdE"
   },
   "source": [
    "## **Project Task: Week 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdYCo3kpA9wd"
   },
   "source": [
    "**Model Selection:**\n",
    "\n",
    "1.   Apply multi-class SVM’s and neural nets.\n",
    "\n",
    "2.   Use possible ensemble techniques like: XGboost + oversampled_multinomial_NB.\n",
    "\n",
    "3.   Assign a score to the sentence sentiment (engineer a feature called sentiment score). Use this engineered feature in the model and check for improvements. Draw insights on the same.\n",
    "\n",
    "**Applying LSTM:**\n",
    "\n",
    "4.   Use LSTM for the previous problem (use parameters of LSTM like top-word, embedding-length, Dropout, epochs, number of layers, etc.)\n",
    "\n",
    "**Hint:** Another variation of LSTM, GRU (Gated Recurrent Units) can be tried as well.\n",
    "\n",
    "5.   Compare the accuracy of neural nets with traditional ML based algorithms.\n",
    "\n",
    "6.   Find the best setting of LSTM (Neural Net) and GRU that can best classify the reviews as positive, negative, and neutral.\n",
    "\n",
    "**Hint:** Use techniques like Grid Search, Cross-Validation and Random Search\n",
    "\n",
    "**Topic Modeling:**\n",
    "\n",
    "7.   Cluster similar reviews.\n",
    "\n",
    "**Note:** Some reviews may talk about the device as a gift-option. Other reviews may be about product looks and some may highlight about its battery and performance. Try naming the clusters.\n",
    "\n",
    "8.   Perform Topic Modeling\n",
    "\n",
    "**Hint:** Use scikit-learn provided Latent Dirchlette Allocation (LDA) and Non-Negative Matrix Factorization (NMF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIvLeIWZacnk"
   },
   "source": [
    "\n",
    "### Apply multi-class SVM’s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nc18SL0meO0H"
   },
   "source": [
    "#### Multi-class SVM’s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lB5UuT2ZcH4W"
   },
   "source": [
    "Source link: https://www.baeldung.com/cs/svm-multiclass-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DLqK5v-cEZl"
   },
   "source": [
    "We’ll create two objects from SVM, to create two different classifiers; one with Polynomial kernel, and another one with RBF kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NgyL4gyPBCwG"
   },
   "outputs": [],
   "source": [
    "#SVM object, with RBF kernel\n",
    "sentiment_SVC_rbf_detection_model = svm.SVC(kernel='rbf', gamma=0.5, C=0.1).fit(X_sm, y_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aM7WjF1tcSNN"
   },
   "outputs": [],
   "source": [
    "#SVM object, with Polynomial kernel\n",
    "sentiment_SVC_poly_detection_model = svm.SVC(kernel='poly', degree=3, C=1).fit(X_sm, y_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TuKuT7Yxc_rc"
   },
   "outputs": [],
   "source": [
    "#check SVM with RBF kernel model for prediction\n",
    "predict_SVC_rbf = sentiment_SVC_rbf_detection_model.predict(test_hidden_tfidf)\n",
    "\n",
    "#delete object\n",
    "del sentiment_SVC_rbf_detection_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbbCgprvdUx2"
   },
   "outputs": [],
   "source": [
    "#check SVM with Polynomial kernel model for prediction\n",
    "predict_SVC_poly = sentiment_SVC_poly_detection_model.predict(test_hidden_tfidf)\n",
    "\n",
    "#delete object\n",
    "del sentiment_SVC_poly_detection_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdjV3stddnt_"
   },
   "source": [
    "##### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sSO6PC_-dotF",
    "outputId": "41e81737-20c1-4eef-be7c-20cd6c7eb806"
   },
   "outputs": [],
   "source": [
    "#Evaluation Metrics for SVM with RBF kernel model\n",
    "print(metrics.classification_report(pd_s_target_test_hidden, predict_SVC_rbf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K346qDMPd5g8",
    "outputId": "faa4f93a-245c-4ef8-b4a9-46034b97a06f"
   },
   "outputs": [],
   "source": [
    "#Evaluation Metrics for SVM with Polynomial kernel model\n",
    "print(metrics.classification_report(pd_s_target_test_hidden, predict_SVC_poly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNurcc7leWTB"
   },
   "source": [
    "### Multi-class neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTXxxiite_eZ"
   },
   "source": [
    "Source link: https://www.tensorflow.org/text/tutorials/text_classification_rnn#create_the_text_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGLVuPJ2fNRw"
   },
   "source": [
    "#### **Create the text encoder**\n",
    "\n",
    "The raw text loaded from *pd_s_feature* needs to be processed before it can be used in a model. The simplest way to process text for training is using the TextVectorization layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Z4q01sd-1db"
   },
   "source": [
    "##### Calculate paramenter for encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4R9N9Ss385c"
   },
   "outputs": [],
   "source": [
    "#get vocabulary from bag of words object\n",
    "lst_bag_of_words_vacabulary = list(obj_bag_of_words.vocabulary_.keys())\n",
    "np_bag_of_words_vacabulary = np.array(lst_bag_of_words_vacabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "peqwI1Lw4ZnZ",
    "outputId": "bee698df-5e58-419f-9b84-f5f6902848ba"
   },
   "outputs": [],
   "source": [
    "#show first 10 words\n",
    "np_bag_of_words_vacabulary[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T5tb-vG34vwb"
   },
   "outputs": [],
   "source": [
    "#find max number of words per review\n",
    "max_nbr_wors_per_review = 0\n",
    "#find index of max number of words per review\n",
    "idx_max_nbr_wors_per_review = 0\n",
    "i = 0\n",
    "for nbr_row_words in all_bag_of_words:\n",
    "  tmp_nbr_words_per_review = nbr_row_words.sum()\n",
    "  if (max_nbr_wors_per_review < tmp_nbr_words_per_review):\n",
    "    max_nbr_wors_per_review = tmp_nbr_words_per_review\n",
    "    idx_max_nbr_wors_per_review = i\n",
    "  i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2wNIxuRo5wAl",
    "outputId": "747b4dfd-889d-44b0-ec91-c7bc3ab962a4"
   },
   "outputs": [],
   "source": [
    "#Show max number of words per review\n",
    "max_nbr_wors_per_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JOuKAadz61Cl",
    "outputId": "fb9f10ab-8d37-4d55-ea82-05ccfb91958e"
   },
   "outputs": [],
   "source": [
    "#Show sentiment of index of max number of words per review\n",
    "pd_s_target_train[idx_max_nbr_wors_per_review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F-Q7YjVD6urp",
    "outputId": "34bf413e-0e25-42d6-efc8-9537c9848b88"
   },
   "outputs": [],
   "source": [
    "#Show review of index of max number of words per review\n",
    "pd_s_feature[idx_max_nbr_wors_per_review]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFzEFTyP_CRF"
   },
   "source": [
    "##### Create text vectorization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0XaAza6xecmD"
   },
   "outputs": [],
   "source": [
    "#Create text vectorization layer\n",
    "\n",
    "#Note: that this vocabulary contains 1 OOV token,\n",
    "#so the effective number of tokens is (max_tokens - 1 - (1 if output_mode == \"int\" else 0))\n",
    "VOCAB_SIZE = np_bag_of_words_vacabulary.shape[0] + 2\n",
    "# max number of words per review is 749, but we put 2000 for rezerv and stop words\n",
    "OUTPUT_SESUENCE_LENGTH = 2000\n",
    "encoder_layer_review = layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    standardize='strip_punctuation',\n",
    "    output_sequence_length=OUTPUT_SESUENCE_LENGTH,\n",
    "    pad_to_max_tokens=False,\n",
    "    vocabulary=np_bag_of_words_vacabulary,# is vocabulary used for machine learning models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D7dEYCh2_M6R",
    "outputId": "32b71fbb-d6da-47f3-ce0a-23f3192536f9"
   },
   "outputs": [],
   "source": [
    "#show first 10 words from layer vacabulary\n",
    "np.array(encoder_layer_review.get_vocabulary())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ljYiqm3tASRR",
    "outputId": "515a9187-875a-4c64-b72e-ceec4cfc5a4a"
   },
   "outputs": [],
   "source": [
    "#Show encoded review with the biggest number of words\n",
    "enc_review_big_nbr_words = encoder_layer_review(pd_s_feature[idx_max_nbr_wors_per_review]).numpy()\n",
    "enc_review_big_nbr_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tdp1J-WKAz5_",
    "outputId": "113c7013-659b-408a-b99e-cfe7b916fd96"
   },
   "outputs": [],
   "source": [
    "#Show decoded review with the biggest number of words\n",
    "print(\" \".join(np.array(encoder_layer_review.get_vocabulary())[enc_review_big_nbr_words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tx63YAj8K07h"
   },
   "source": [
    "#### Create the neural network model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eKLd8E2MK7PO"
   },
   "outputs": [],
   "source": [
    "input_embeding_layer_nn = Input(shape=(2000,), dtype=np.uint32)\n",
    "x = layers.Embedding(\n",
    "                    input_dim=VOCAB_SIZE,\n",
    "                    output_dim=64,\n",
    "                    mask_zero=True,\n",
    "                    # Use masking to handle the variable sequence lengths\n",
    "                    input_length=OUTPUT_SESUENCE_LENGTH,\n",
    "                )(input_embeding_layer_nn)\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.MaxPooling1D(pool_size=2,\n",
    "   strides=2, padding='valid')(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.MaxPooling1D(pool_size=2,\n",
    "   strides=2, padding='valid')(x)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dense(1, activation='relu')(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(rate=0.4)(x)\n",
    "out_layer_nn = layers.Dense(len(pd_s_target_train.unique()), activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uDqLWGT5KjeE"
   },
   "outputs": [],
   "source": [
    "#create neural network model\n",
    "embeding_model_nn = Model(input_embeding_layer_nn, out_layer_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5p-THcJGRJab"
   },
   "source": [
    "#### Compile the model\n",
    "\n",
    "We will, use the `tf.keras.optimizers.Adam` optimizer and `categorical_crossentropy` loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Mw0-sxpQ-ZD"
   },
   "outputs": [],
   "source": [
    "embeding_model_nn.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ERzjKTiRTsR"
   },
   "source": [
    "#### Model summary\n",
    "\n",
    "View all the layers of the network using the Keras `Model.summary` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1hUGzMUORXg_",
    "outputId": "040cf459-c811-4acd-8fa3-02c403539981"
   },
   "outputs": [],
   "source": [
    "embeding_model_nn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HBD6IfZSs9Z"
   },
   "source": [
    "#### Train and visualize the model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9hazXoKb_y-m"
   },
   "outputs": [],
   "source": [
    "#encode train data\n",
    "encode_feature_train = encoder_layer_review(pd_s_feature_train)\n",
    "#encode test data hidden\n",
    "encode_feature_test_hidden = encoder_layer_review(pd_s_target_test_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k527wHV8AHZI"
   },
   "outputs": [],
   "source": [
    "#Oversampling is used to tackle the class imbalance problem.\n",
    "smote = SMOTE(random_state=42)\n",
    "X_sm_encode, y_sm_encode = smote.fit_resample(encode_feature_train, pd_s_target_train)\n",
    "\n",
    "#delete object\n",
    "del smote\n",
    "\n",
    "#binary encode target data\n",
    "class_samples_ = pd_s_target_train.unique()\n",
    "#binary encode target train data\n",
    "np_target_train = label_binarize(y_sm_encode, classes=class_samples_)\n",
    "#binary encode target test data hidden\n",
    "np_target_test_hidden = label_binarize(pd_s_target_test_hidden, classes=class_samples_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IfNEMb1-SzHK"
   },
   "outputs": [],
   "source": [
    "epochs=20\n",
    "history = embeding_model_nn.fit(\n",
    "                        x = X_sm_encode,\n",
    "                        y = np_target_train,\n",
    "                        validation_data=(encode_feature_test_hidden, np_target_test_hidden),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=20,\n",
    "                        callbacks=[PlotLossesKerasTF()]\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l6_SZyPoz8j_"
   },
   "outputs": [],
   "source": [
    "#embeding_model_nn.load_weights(\"nn_model_weights.h5\", by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0LzF1pfYRza"
   },
   "outputs": [],
   "source": [
    "filepath = 'nn_model_weights.h5'\n",
    "embeding_model_nn.save_weights(filepath, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaA97I2pQBI0"
   },
   "source": [
    "#### Predict neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SBMAtjBIQNu7"
   },
   "outputs": [],
   "source": [
    "input_layer_nn = Input(shape=(1,), dtype=tf.string)\n",
    "input_encoder_layer_nn = encoder_layer_review(input_layer_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "puDCx8gUQHvp"
   },
   "outputs": [],
   "source": [
    "model_nn = Model(input_layer_nn, embeding_model_nn(input_encoder_layer_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ue5evvBtqP2M",
    "outputId": "09a33e50-2dca-4a46-8528-52a61d5b8a1a"
   },
   "outputs": [],
   "source": [
    "predict_nn = model_nn.predict(pd_s_feature_test_hidden)\n",
    "predict_nn = class_samples_[[np.argmax(i) for i in predict_nn]]\n",
    "\n",
    "#delete models\n",
    "del embeding_model_nn\n",
    "del model_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qEuuvO0-qo6P",
    "outputId": "287971a0-21d9-4bd8-8fa9-cba89987705b"
   },
   "outputs": [],
   "source": [
    "#Evaluation Metrics for neural network model\n",
    "print(metrics.classification_report(pd_s_target_test_hidden, predict_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOy5ZYPHXaZP"
   },
   "source": [
    "### Ensemble techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZD50B0RXjtr"
   },
   "outputs": [],
   "source": [
    "dict_pred = {'NB': predict_NB,\n",
    "             'RF': predict_RF_best_rand_params,\n",
    "             'XGB': predict_XGB_best_rand_params,\n",
    "             'SVC_rbf': predict_SVC_rbf,\n",
    "             'SVC_poly': predict_SVC_poly,\n",
    "             'NN': predict_nn,\n",
    "             }\n",
    "pd_s_pred_mode = pd.DataFrame(dict_pred).T.mode().T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8hi2ljaHbIy6",
    "outputId": "1c46b8bf-737c-486f-c7d9-e6bfe148a686"
   },
   "outputs": [],
   "source": [
    "#Evaluation Metrics mode of Naive Bayes, Random Forest, Xgboost,\n",
    "#SVM with RBF kernel, SVM with Polynomial kernel, Neural network\n",
    "print(metrics.classification_report(pd_s_target_test_hidden, pd_s_pred_mode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JaiJvhuhtSp"
   },
   "source": [
    "### RNN models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "859kjLH_irT8"
   },
   "source": [
    "##### Make encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gM4sFoAOhwl5"
   },
   "outputs": [],
   "source": [
    "#find vocabulary size\n",
    "VOCAB_SIZE_TF_TXT_VECT = 20000\n",
    "encoder = layers.TextVectorization(\n",
    "                                  max_tokens=VOCAB_SIZE_TF_TXT_VECT,\n",
    "                                  output_mode='int',\n",
    "                                  standardize='strip_punctuation',\n",
    "                                  output_sequence_length=1,\n",
    "                                  pad_to_max_tokens=False\n",
    "                                  )\n",
    "encoder.adapt(pd_s_feature.map(lambda text: text))\n",
    "VOCAB_SIZE_TF_TXT_VECT = len(encoder.get_vocabulary())+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZHZBN9ziT0p"
   },
   "outputs": [],
   "source": [
    "#make encoder text vectorization layer\n",
    "enc_review_tf_txt_vect = layers.TextVectorization(\n",
    "                                  max_tokens=VOCAB_SIZE_TF_TXT_VECT,\n",
    "                                  output_mode='int',\n",
    "                                  standardize='strip_punctuation',\n",
    "                                  output_sequence_length=2000,\n",
    "                                  pad_to_max_tokens=False\n",
    "                                  )\n",
    "enc_review_tf_txt_vect.adapt(pd_s_feature.map(lambda text: text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83i3THEElMBx"
   },
   "source": [
    "##### Encode train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5iRh9oc0lHNn"
   },
   "outputs": [],
   "source": [
    "encode_feature_train = enc_review_tf_txt_vect(pd_s_feature_train)\n",
    "encode_feature_test_hidden = enc_review_tf_txt_vect(pd_s_target_test_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVQgF0o7lg7q"
   },
   "source": [
    "##### Oversample encoded with tensorflow text vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_NyJp5iplZBk"
   },
   "outputs": [],
   "source": [
    "#Oversampling is used to tackle the class imbalance problem.\n",
    "smote = SMOTE(random_state=42)\n",
    "X_sm_tf_vec, y_sm_tf_vec = smote.fit_resample(encode_feature_train, pd_s_target_train)\n",
    "\n",
    "#delete object\n",
    "del smote\n",
    "\n",
    "#One-hote encoding\n",
    "class_samples_ = pd_s_target_train.unique()\n",
    "np_target_train_rnn = label_binarize(y_sm_tf_vec, classes=class_samples_)\n",
    "np_target_test_hidden_rnn = label_binarize(pd_s_target_test_hidden, classes=class_samples_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPP7TmN3Ryn8"
   },
   "source": [
    "##### Input string model rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lZHqe0x_R5br"
   },
   "outputs": [],
   "source": [
    "input_layer_rnn = Input(shape=(1,), dtype=tf.string)\n",
    "input_encoder_layer_rnn = enc_review_tf_txt_vect(input_layer_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeFl7MiZhzFK"
   },
   "source": [
    "#### Multiclass LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjIwCAy_i0de"
   },
   "source": [
    "##### Build LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6yME06osipie"
   },
   "outputs": [],
   "source": [
    "input_embeding_layer_lstm = Input(shape=(2000,), dtype=np.uint32)\n",
    "x = layers.Embedding(\n",
    "                    input_dim=VOCAB_SIZE_TF_TXT_VECT,\n",
    "                    output_dim=64,\n",
    "                    mask_zero=True,\n",
    "                    # Use masking to handle the variable sequence lengths\n",
    "                    input_length=2000,\n",
    "                )(input_embeding_layer_lstm)\n",
    "x = layers.Bidirectional(layers.LSTM(64,  return_sequences=True))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.LSTM(32)(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(rate=0.4)(x)\n",
    "out_layer_lstm = layers.Dense(len(pd_s_target_train.unique()), activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vYzzczLijXG1"
   },
   "outputs": [],
   "source": [
    "#Do model with input embeding layer\n",
    "embeding_model_lstm = Model(input_embeding_layer_lstm, out_layer_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRAUFLkgjm_d"
   },
   "source": [
    "##### Compile model\n",
    "\n",
    "We will, use the 'tf.keras.optimizers.Adam' optimizer and 'categorical_crossentropy' loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QegigocbjpJb"
   },
   "outputs": [],
   "source": [
    "embeding_model_lstm.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLNGVfmqkl3d"
   },
   "source": [
    "##### Model summary\n",
    "\n",
    "View all the layers of the network using the Keras `Model.summary` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k_FDZTlPknrB",
    "outputId": "930ed32e-7976-4e23-efa6-d1c63a8cca1e"
   },
   "outputs": [],
   "source": [
    "embeding_model_lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKtdzzovk58d"
   },
   "source": [
    "##### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uoY61exvk3nZ"
   },
   "outputs": [],
   "source": [
    "epochs=10\n",
    "history = embeding_model_lstm.fit(\n",
    "                        x = X_sm_tf_vec,\n",
    "                        y = np_target_train_rnn,\n",
    "                        validation_data=(encode_feature_test_hidden, np_target_test_hidden_rnn),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=20,\n",
    "                        callbacks=[PlotLossesKerasTF()]\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p8zvSD2v4z8c"
   },
   "outputs": [],
   "source": [
    "#embeding_model_lstm.load_weights(\"lstm_model_weights.h5\", by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yFnxuzQxYfGh"
   },
   "outputs": [],
   "source": [
    "filepath = 'lstm_model_weights.h5'\n",
    "embeding_model_lstm.save_weights(filepath, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZJbbB3AUAa1"
   },
   "source": [
    "##### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TubBy0qvUI8R"
   },
   "outputs": [],
   "source": [
    "model_lstm = Model(input_layer_rnn, embeding_model_lstm(input_encoder_layer_rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u5KYk7YHUTJC",
    "outputId": "962dc9bd-2337-4463-d297-b73a3a9c544b"
   },
   "outputs": [],
   "source": [
    "predict_lstm = model_lstm.predict(pd_s_feature_test_hidden)\n",
    "predict_lstm = class_samples_[[np.argmax(i) for i in predict_lstm]]\n",
    "\n",
    "#delete models\n",
    "del embeding_model_lstm\n",
    "del model_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vs7sktsLUegW",
    "outputId": "a7b34893-e81a-4def-aca8-8e38a45da6ba"
   },
   "outputs": [],
   "source": [
    "#Evaluation Metrics for LSTM model\n",
    "print(metrics.classification_report(pd_s_target_test_hidden, predict_lstm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2-LUcsNjDF9"
   },
   "source": [
    "#### Multiclass GRU model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJ56gHTkjKKf"
   },
   "source": [
    "##### Build GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dcw6fQ7ji97c"
   },
   "outputs": [],
   "source": [
    "input_embeding_layer_gru = Input(shape=(2000,), dtype=np.uint32)\n",
    "x = layers.Embedding(\n",
    "                    input_dim=VOCAB_SIZE_TF_TXT_VECT,\n",
    "                    output_dim=64,\n",
    "                    mask_zero=True,\n",
    "                    # Use masking to handle the variable sequence lengths\n",
    "                    input_length=2000,\n",
    "                )(input_embeding_layer_gru)\n",
    "x = layers.Bidirectional(layers.GRU(128,  return_sequences=True))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.GRU(128)(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(rate=0.4)(x)\n",
    "out_layer_gru = layers.Dense(len(pd_s_target_train.unique()), activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQocHtgMkHDY"
   },
   "outputs": [],
   "source": [
    "#Do model with input embeding layer\n",
    "embeding_model_gru = Model(input_embeding_layer_gru, out_layer_gru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZskbyztqkXlY"
   },
   "source": [
    "##### Compile model\n",
    "\n",
    "We will, use the 'tf.keras.optimizers.Adam' optimizer and 'categorical_crossentropy' loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "To4MkJaVkVgy"
   },
   "outputs": [],
   "source": [
    "embeding_model_gru.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0r7eAdV9keVQ"
   },
   "source": [
    "##### Model summary\n",
    "\n",
    "View all the layers of the network using the Keras `Model.summary` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Yx5sDVHkqsY",
    "outputId": "0ecf84bb-3f6c-4b43-e23d-bc36d006912a"
   },
   "outputs": [],
   "source": [
    "embeding_model_gru.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdf0B8sLmU65"
   },
   "source": [
    "##### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SUDGDRHcmSTr"
   },
   "outputs": [],
   "source": [
    "epochs=10\n",
    "history = embeding_model_gru.fit(\n",
    "                        x = X_sm_tf_vec,\n",
    "                        y = np_target_train_rnn,\n",
    "                        validation_data=(encode_feature_test_hidden, np_target_test_hidden_rnn),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=20,\n",
    "                        callbacks=[PlotLossesKerasTF()]\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EwRl3STOb7xd"
   },
   "outputs": [],
   "source": [
    "#embeding_model_gru.load_weights(\"gru_model_weights.h5\", by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kj-cJI2w5zgy"
   },
   "outputs": [],
   "source": [
    "filepath = 'gru_model_weights.h5'\n",
    "embeding_model_gru.save_weights(filepath, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4yY2dJmRWZb"
   },
   "source": [
    "##### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jxBeOYgRdSt"
   },
   "outputs": [],
   "source": [
    "model_gru = Model(input_layer_rnn, embeding_model_gru(input_encoder_layer_rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nzK90zPmS4NL",
    "outputId": "19deeff3-c418-4240-bcb7-caf6e3362918"
   },
   "outputs": [],
   "source": [
    "predict_gru = model_gru.predict(pd_s_feature_test_hidden)\n",
    "predict_gru = class_samples_[[np.argmax(i) for i in predict_gru]]\n",
    "\n",
    "#delete models\n",
    "del embeding_model_gru\n",
    "del model_gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "St_Bvva9TKGq",
    "outputId": "7b466122-eb2f-4ca6-efeb-0bf8692f86bc"
   },
   "outputs": [],
   "source": [
    "#Evaluation Metrics for GRU model\n",
    "print(metrics.classification_report(pd_s_target_test_hidden, predict_gru))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2c7CyJQfJxk"
   },
   "source": [
    "### Compare the accuracy of neural nets with traditional ML based algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPDoAfhjglGb"
   },
   "source": [
    "#### Machine learning evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G93Utp0jf7K7"
   },
   "outputs": [],
   "source": [
    "#Evaluation Metrics Naive Bayes Clasifier\n",
    "print(metrics.classification_report(pd_s_target_test_hidden, predict_NB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TUeZ2uhDfLxv"
   },
   "outputs": [],
   "source": [
    "#Evaluation Metrics Random Forest Clasifier\n",
    "print(metrics.classification_report(pd_s_target_test_hidden, predict_RF_best_rand_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0fPevi9lgJwk"
   },
   "outputs": [],
   "source": [
    "#Evaluation Metrics eXtreme Gradient Boosting\n",
    "print(metrics.classification_report(pd_s_target_test_hidden, predict_XGB_best_rand_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Re65VV24gbwV"
   },
   "outputs": [],
   "source": [
    "#Evaluation Metrics for SVM with RBF kernel model\n",
    "print(metrics.classification_report(pd_s_target_test_hidden, predict_SVC_rbf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYJOWQ89geXi"
   },
   "outputs": [],
   "source": [
    "#Evaluation Metrics for SVM with Polynomial kernel model\n",
    "print(metrics.classification_report(pd_s_target_test_hidden, predict_SVC_poly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-p94o7ZgtBD"
   },
   "source": [
    "#### Neuronal network evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DC5FHFEYg8qh",
    "outputId": "0a7131d9-6540-4b59-eb50-9c8d4dbeddd8"
   },
   "outputs": [],
   "source": [
    "#Evaluation Metrics for neural network model\n",
    "print(metrics.classification_report(pd_s_target_test_hidden, predict_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DeNPbXNWg9Zd",
    "outputId": "3d707ca6-2e0f-46ee-f640-897aa2f6bc47"
   },
   "outputs": [],
   "source": [
    "#Evaluation Metrics for LSTM model\n",
    "print(metrics.classification_report(pd_s_target_test_hidden, predict_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_WR7xVx8hJ7Z",
    "outputId": "24fb12c9-186c-41ac-c102-e984113c4114"
   },
   "outputs": [],
   "source": [
    "#Evaluation Metrics for GRU model\n",
    "print(metrics.classification_report(pd_s_target_test_hidden, predict_gru))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2INtDKNLhL9L"
   },
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30qEHpxxhl0O"
   },
   "source": [
    "The result prediction of machine learning techniques are more bigger that neuronal network solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SU3KgjxWh8bn"
   },
   "source": [
    "### Fine tuning of hyperparameter of deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hqm_Bhii0Cy"
   },
   "source": [
    "##### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6YKRx4ZDiI5B"
   },
   "outputs": [],
   "source": [
    "def create_model(neurons_bid_gru, neurons_dense, nbr_out_net, optimizer='adam', activation='relu'):\n",
    "  input_embeding_layer_gru = Input(shape=(2000,), dtype=np.uint32)\n",
    "  x = layers.Embedding(\n",
    "                      input_dim=VOCAB_SIZE_TF_TXT_VECT,\n",
    "                      output_dim=64,\n",
    "                      mask_zero=True,\n",
    "                      # Use masking to handle the variable sequence lengths\n",
    "                      input_length=2000,\n",
    "                      )(input_embeding_layer_gru)\n",
    "  x = layers.Bidirectional(layers.GRU(neurons_bid_gru,  return_sequences=True))(x)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  x = tf.keras.layers.GRU(32)(x)\n",
    "  x = layers.Dense(neurons_dense, activation='relu')(x)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  x = layers.Dropout(rate=0.4)(x)\n",
    "  out_layer_gru = layers.Dense(nbr_out_net, activation=activation)(x)\n",
    "\n",
    "  #create neural network model\n",
    "  embeding_model_gru = Model(input_embeding_layer_gru, out_layer_gru)\n",
    "\n",
    "  # Compile model\n",
    "  embeding_model_gru.compile(loss='categorical_crossentropy',\n",
    "                              optimizer=optimizer, metrics=['accuracy'])\n",
    "  return embeding_model_gru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xW5RJ78NjfOd"
   },
   "source": [
    "##### Define the grid search parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGQcmeSLi8fp"
   },
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "batch_size = [10, 40, 80]\n",
    "optimizer = ['SGD', 'Adam']\n",
    "learn_rate = [0.01, 0.3]\n",
    "momentum = [0.0, 0.4, 0.8]\n",
    "neurons_bid_gru = [32, 64]\n",
    "neurons_dense = [64, 128]\n",
    "activation = ['softmax', 'sigmoid']\n",
    "nbr_out_net = [int(len(pd_s_target_train.unique()))]\n",
    "\n",
    "param_grid = dict(\n",
    "                  batch_size=batch_size,\n",
    "                  optimizer__learning_rate=learn_rate,\n",
    "                  optimizer__momentum=momentum,\n",
    "                  model__activation=activation,\n",
    "                  model__optimizer=optimizer,\n",
    "                  model__neurons_bid_gru=neurons_bid_gru,\n",
    "                  model__neurons_dense=neurons_dense,\n",
    "                  model__nbr_out_net=nbr_out_net,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Yt-7CS-jl6j"
   },
   "source": [
    "##### Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fcd46GeejDcj",
    "outputId": "4b0ecc93-1783-4584-c2a1-e73969bb3d8d"
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "model_GRU = KerasClassifier(model=create_model, epochs=1)\n",
    "# define the grid search parameters\n",
    "\n",
    "gru_hyper_tune_random = RandomizedSearchCV(estimator = model_GRU, param_distributions = param_grid,\n",
    "                                 n_iter = 2, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "pred_gru_hyper_tune_random = gru_hyper_tune_random.fit(X_sm_tf_vec, np_target_train_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "riZSemqHjGgR",
    "outputId": "33efa65d-ca22-4d27-ac9f-a6fd8d128fcd"
   },
   "outputs": [],
   "source": [
    "#show best pamameters\n",
    "pred_gru_hyper_tune_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53x8P6iyjqi9"
   },
   "source": [
    "##### Prediction of best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qIwif-F7jJv5",
    "outputId": "7ee47e38-2e28-4a16-cd1e-d153f1ea81f2"
   },
   "outputs": [],
   "source": [
    "#predict sentiment with best random paramenters\n",
    "predict_GRU_best_rand_params = pred_gru_hyper_tune_random.best_estimator_.predict(encode_feature_test_hidden)\n",
    "predict_GRU_best_rand_params = class_samples_[[np.argmax(i) for i in predict_GRU_best_rand_params]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PCPCi8YojNJX",
    "outputId": "6369c71c-d297-4f1f-c889-05cdc27ad214"
   },
   "outputs": [],
   "source": [
    "#Evaluation Metrics\n",
    "print(metrics.classification_report(pd_s_target_test_hidden, predict_GRU_best_rand_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uj0NLtogmbl"
   },
   "source": [
    "### Perform Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9VlYG1Hg7-b1"
   },
   "outputs": [],
   "source": [
    "n_components = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kchuTauV7igQ"
   },
   "source": [
    "##### TFIDF transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZqHhsjSbWA60"
   },
   "outputs": [],
   "source": [
    "# apply transform method for the bag of words of all data\n",
    "all_data_bag_of_words = obj_bag_of_words.transform(pd_s_feature)\n",
    "# apply tfidf transformer for all bag of words into it (transformed version)\n",
    "all_data_tfidf = obj_tfidf.transform(all_data_bag_of_words)\n",
    "\n",
    "# apply transform method for the bag of words of train data\n",
    "train_bag_of_words = obj_bag_of_words.transform(pd_s_feature_train)\n",
    "# apply tfidf transformer for train bag of words into it (transformed version)\n",
    "train_tfidf = obj_tfidf.transform(train_bag_of_words)\n",
    "\n",
    "# apply transform method for the bag of words of test data hidden\n",
    "test_hidden_bag_of_words = obj_bag_of_words.transform(pd_s_feature_test_hidden)\n",
    "# apply tfidf transformer for train bag of words into it (transformed version)\n",
    "test_hidden_tfidf = obj_tfidf.transform(test_hidden_bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFByPpIG7u-h"
   },
   "source": [
    "##### Build neuronal network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OKWC8dLcXH8V"
   },
   "outputs": [],
   "source": [
    "input_nn_TM = Input(shape=(n_components,), dtype=np.float32)\n",
    "x = layers.Dense(182, activation='relu')(input_nn_TM)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dropout(rate=0.5)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = layers.Dropout(rate=0.3)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dense(16, activation='relu')(x)\n",
    "x = layers.Dropout(rate=0.5)(x)\n",
    "x = layers.Dense(8, activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(rate=0.6)(x)\n",
    "out_nn_TM = layers.Dense(len(pd_s_target_train.unique()), activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESmRNKy4gzwp"
   },
   "source": [
    "#### Latent Dirchlette Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ilBjnpvhuv2"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXeDgh23zL4u"
   },
   "source": [
    "##### LDA transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ltoyyXZLhldx",
    "outputId": "453e68b6-b6ac-4258-977d-39f3c6179dc8"
   },
   "outputs": [],
   "source": [
    "# This produces a feature matrix of token counts, similar to what\n",
    "# CountVectorizer would produce on text.\n",
    "lda = LatentDirichletAllocation(n_components=n_components, random_state=0)\n",
    "lda.fit(all_data_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9p6g_vRmnIaM"
   },
   "outputs": [],
   "source": [
    "# get topics for train data\n",
    "X_train_lda = lda.transform(train_tfidf)\n",
    "# get topics for test hidden\n",
    "X_test_hidden_lda = lda.transform(test_hidden_tfidf)\n",
    "\n",
    "#Oversampling is used to tackle the class imbalance problem.\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_sm_lda, y_train_sm = smote.fit_resample(X_train_lda, pd_s_target_train)\n",
    "del smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Bk7Esb0dg4J"
   },
   "outputs": [],
   "source": [
    "class_samples_ = pd_s_target_train.unique()\n",
    "y_train_sm_one_hot = label_binarize(y_train_sm, classes=class_samples_)\n",
    "y_test_hidden_one_hot = label_binarize(pd_s_target_test_hidden, classes=class_samples_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7xNhdPHyALM"
   },
   "source": [
    "##### Build neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QaMZShNhcH1S"
   },
   "outputs": [],
   "source": [
    "model_nn_lda_TM = Model(input_nn_TM, out_nn_TM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UWH6LmQ0cyPp"
   },
   "outputs": [],
   "source": [
    "model_nn_lda_TM.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32u0knETx4Xh"
   },
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 727
    },
    "id": "AK_Nbt2hc840",
    "outputId": "b6e795d7-d7bd-45ec-ef00-71802b44de5a"
   },
   "outputs": [],
   "source": [
    "epochs=60\n",
    "history = model_nn_lda_TM.fit(\n",
    "                        x = X_train_sm_lda,\n",
    "                        y = y_train_sm_one_hot,\n",
    "                        validation_data=(X_test_hidden_lda, y_test_hidden_one_hot),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=40,\n",
    "                        callbacks=[PlotLossesKerasTF()]\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LKLfl3Hex1Y0"
   },
   "source": [
    "##### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VOBZfQUjglcv"
   },
   "outputs": [],
   "source": [
    "#predict sentiment with LDA decomposition\n",
    "predict_nn_lda_TM = model_nn_lda_TM.predict(X_test_hidden_lda)\n",
    "predict_nn_lda_TM = class_samples_[[np.argmax(i) for i in predict_nn_lda_TM]]\n",
    "\n",
    "#delete model\n",
    "del model_nn_lda_TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BLOx6k1Ug5V5",
    "outputId": "816aebed-a15d-4c70-a72a-df760dc146cb"
   },
   "outputs": [],
   "source": [
    "#Evaluation Metrics for LDA decomposition\n",
    "print(metrics.classification_report(pd_s_target_test_hidden, predict_nn_lda_TM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVhrb40GHv7u"
   },
   "source": [
    "#### Non-Negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKUTZDfnxfYI"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilzFuISn8OHf"
   },
   "source": [
    "##### NMF Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oWNRLfD6Hxsk",
    "outputId": "6867253e-e810-4404-fa24-0b3aa9d72616"
   },
   "outputs": [],
   "source": [
    "model_nmf = NMF(n_components=n_components, init='random', random_state=0)\n",
    "model_nmf.fit(all_data_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lh7Dat8ty2Y3"
   },
   "outputs": [],
   "source": [
    "# get topics for train data\n",
    "X_train_nmf = model_nmf.transform(train_tfidf)\n",
    "# get topics for test hidden\n",
    "X_test_hidden_nmf = model_nmf.transform(test_hidden_tfidf)\n",
    "\n",
    "#Oversampling is used to tackle the class imbalance problem.\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_sm_nmf, y_train_sm = smote.fit_resample(X_train_nmf, pd_s_target_train)\n",
    "del smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cAlfEOU3zCN7"
   },
   "outputs": [],
   "source": [
    "class_samples_ = pd_s_target_train.unique()\n",
    "y_train_sm_one_hot = label_binarize(y_train_sm, classes=class_samples_)\n",
    "y_test_hidden_one_hot = label_binarize(pd_s_target_test_hidden, classes=class_samples_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGuuVcLn8Yz0"
   },
   "source": [
    "##### Build nn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "klik85dl39If"
   },
   "outputs": [],
   "source": [
    "model_nn_nmf_TM = Model(input_nn_TM, out_nn_TM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7R74sAxv4DjP"
   },
   "outputs": [],
   "source": [
    "model_nn_nmf_TM.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFcznqum8gHD"
   },
   "source": [
    "##### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4NOL-Vue4NxU",
    "outputId": "19f520e3-b090-4586-d569-1a50beb27b30"
   },
   "outputs": [],
   "source": [
    "epochs=90\n",
    "history = model_nn_nmf_TM.fit(\n",
    "                        x = X_train_sm_nmf,\n",
    "                        y = y_train_sm_one_hot,\n",
    "                        validation_data=(X_test_hidden_nmf, y_test_hidden_one_hot),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=40,\n",
    "                        callbacks=[PlotLossesKerasTF()]\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_oI2Ee0p8kjZ"
   },
   "source": [
    "##### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cTNhz38p5l-n",
    "outputId": "5fd83652-f039-429f-c43e-b2358a9b93d0"
   },
   "outputs": [],
   "source": [
    "#predict sentiment with NMF decomposition\n",
    "predict_nn_nmf_TM = model_nn_nmf_TM.predict(X_test_hidden_nmf)\n",
    "predict_nn_nmf_TM = class_samples_[[np.argmax(i) for i in predict_nn_nmf_TM]]\n",
    "\n",
    "#delete model\n",
    "#del model_nn_nmf_TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5CSC9n6u519l",
    "outputId": "f3f026a9-45b4-4b7e-fb53-7176d63253a8"
   },
   "outputs": [],
   "source": [
    "#Evaluation Metrics for NMF decomposition\n",
    "print(metrics.classification_report(pd_s_target_test_hidden, predict_nn_nmf_TM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EyQ3FvLp9KeZ"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aeBzZyn9NXB"
   },
   "source": [
    "Working on this project:\n",
    "\n",
    "- Perform an EDA on the dataset\n",
    "- Convert the reviews in Tf-Idf score\n",
    "- Text procesing (drop stopwords, drop punctuation and lemmatization)\n",
    "- Implement several ML algorithms(Naive Bayes, Random Forest, Xgboost and SVM’s)\n",
    "- Tackle the class of imbalance problem with SMOTE\n",
    "- Use the following metrices for evaluating model performance: precision, recall, F1-score, AUC-ROC curve\n",
    "- Use fine-tuning parameter for ML algoritm like RandomizedSearchCV\n",
    "- Use ensemble techniques like: XGboost + NB + RF + SVM.\n",
    "- Use LSTM, GRU and NN deep learning model\n",
    "- Use fine-tuning parameter for DL models like RandomizedSearchCV\n",
    "- Provided Latent Dirchlette Allocation (LDA) and Non-Negative Matrix Factorization (NMF)\n",
    "\n",
    "\n",
    "I have come to the conclusion that some ML algorithms offer much better results to predict unbalanced data, but if we use some decomposition algorithms like LDA and NMF, for NN learning algorithms we can obtain results comparable to ML models. We can use fine-tuning parameter to find best parameter for best prediction."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "6rcqJtC5huvt",
    "doa2gSCNhi-x",
    "JnNz6asOgw6z",
    "G5gjMyJfjNxp",
    "oZCB-u8kgrgs",
    "QPN71Sjvi2-Z",
    "IEAJ_O_ljCKy",
    "7mL90LlCjury",
    "7n72fYju5J69",
    "PrRbXzIbo1Jm",
    "N8L4dbZO9q5y",
    "tPssdNWOSzpq",
    "sv_C1O9icS0x",
    "4iyU4qVqAcCA",
    "b4aYt-h_dNhu",
    "c2wxc1e1D5wm",
    "EfYOTE8iEbwQ",
    "GGFdL5KeWmAu",
    "1KKcEoDiYY4C",
    "58dpf__FezPh",
    "8bHZHPXMY-is",
    "K37edybA02pF",
    "ug5iXO4n13Cj",
    "Px4_YSMpDohQ",
    "NIvLeIWZacnk",
    "Nc18SL0meO0H",
    "UdjV3stddnt_",
    "qNurcc7leWTB",
    "CGLVuPJ2fNRw",
    "4Z4q01sd-1db",
    "PFzEFTyP_CRF",
    "tx63YAj8K07h",
    "5p-THcJGRJab",
    "6ERzjKTiRTsR",
    "GOy5ZYPHXaZP",
    "5JaiJvhuhtSp",
    "859kjLH_irT8",
    "83i3THEElMBx",
    "TVQgF0o7lg7q",
    "BBx3rOXFmC57",
    "oPP7TmN3Ryn8",
    "jeFl7MiZhzFK",
    "FjIwCAy_i0de",
    "lRAUFLkgjm_d",
    "sLNGVfmqkl3d",
    "mKtdzzovk58d",
    "5ZJbbB3AUAa1",
    "vJ56gHTkjKKf",
    "ZskbyztqkXlY",
    "0r7eAdV9keVQ",
    "T4yY2dJmRWZb",
    "I2c7CyJQfJxk",
    "1hqm_Bhii0Cy",
    "m3WqjgyFnPLj"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
